# Getting Started with HPC Sweep Manager

This guide will help you get up and running with HPC Sweep Manager (HSM) across all execution modes.

## üîß Installation

### Prerequisites
- Python 3.8 or higher
- SSH access to remote machines (for remote/distributed modes)
- PBS/Torque or Slurm (for HPC mode)

### Install from PyPI
```bash
pip install hpc-sweep-manager
```

### Development Installation
```bash
git clone https://github.com/yourusername/hpc-sweep-manager.git
cd hpc-sweep-manager
pip install -e .
```

## üéØ Quick Start: Choose Your Mode

HSM supports four execution modes. Choose based on your needs:

| Mode | Use Case | Setup Complexity | Scale |
|------|----------|------------------|-------|
| **Local** | Development, testing | ‚≠ê Simple | 1-10 jobs |
| **Remote** | Utilize remote GPU/server | ‚≠ê‚≠ê Medium | 10-100 jobs |
| **Distributed** | Multi-machine experiments | ‚≠ê‚≠ê‚≠ê Advanced | 100-1000 jobs |
| **HPC** | Cluster computing | ‚≠ê‚≠ê Medium | 1000+ jobs |

## üöÄ Local Mode - Quick Start

Perfect for development and small experiments on your local machine.

### 1. Initialize Project
```bash
cd /path/to/your/ml/project
hsm init
```

HSM will auto-detect your Python environment and training script:
```
‚úì Detected Python: /opt/conda/bin/python
‚úì Found training script: train.py
‚úì Created configuration: sweeps/hsm_config.yaml
```

### 2. Configure Parameters
Edit `sweeps/sweep.yaml`:
```yaml
sweep:
  grid:
    seed: [1, 2, 3]
    model.hidden_size: [128, 256]
    optimizer.lr: [0.001, 0.01]
```

### 3. Run Local Sweep
```bash
# Small test run
hsm sweep run --mode local --parallel-jobs 2 --max-runs 6

# With real-time output
hsm sweep run --mode local --parallel-jobs 2 --show-output
```

## üåê Remote Mode - Quick Start

Execute jobs on a remote machine via SSH.

### 1. Setup SSH Access
Ensure passwordless SSH to your remote machine:
```bash
ssh-copy-id user@remote-server.edu
ssh user@remote-server.edu  # Should not prompt for password
```

### 2. Add Remote Machine
```bash
hsm remote add gpu-server remote-server.edu --key ~/.ssh/id_rsa
```

### 3. Test Connection
```bash
hsm remote test gpu-server
```

Expected output:
```
‚úì gpu-server: Configuration discovered successfully
  Host: remote-server.edu
  Python: /home/user/miniconda3/bin/python
  Project Root: /home/user/my-project
  Train Script: train.py
  Max Jobs: 4
```

### 4. Run Remote Sweep
```bash
hsm sweep run --mode remote --remote gpu-server --max-runs 12
```

## üåü Distributed Mode - Quick Start

Orchestrate jobs across multiple machines simultaneously.

### 1. Initialize Distributed Computing
```bash
hsm distributed init
```

### 2. Add Multiple Compute Sources
```bash
# Add remote machines
hsm distributed add gpu-server1 gpu1.university.edu --max-jobs 2
hsm distributed add gpu-server2 gpu2.university.edu --max-jobs 4
hsm distributed add workstation ws.lab.edu --max-jobs 1

# Local machine is automatically included
```

### 3. Test All Sources
```bash
hsm distributed test --all
```

### 4. Run Distributed Sweep
```bash
hsm sweep run --mode distributed
```

HSM will automatically balance jobs across all available sources:
```
‚úì 3 compute sources ready for distributed execution
Starting distributed execution across 3 sources...
[1/24] Job task_001 submitted to local
[2/24] Job task_002 submitted to gpu-server1  
[3/24] Job task_003 submitted to gpu-server2
...
Progress: 18/24 (75.0%) - ‚úì 15 completed, ‚úó 0 failed
```

## üèõÔ∏è HPC Mode - Quick Start

Submit array jobs to PBS/Torque or Slurm clusters.

### 1. Verify HPC System
```bash
# PBS/Torque
qstat --version

# Slurm  
squeue --version
```

HSM auto-detects your scheduler.

### 2. Configure HPC Settings
Edit `sweeps/hsm_config.yaml`:
```yaml
hpc:
  default_walltime: "04:00:00"
  default_resources: "select=1:ncpus=4:mem=16gb"  # PBS
  # default_resources: "--nodes=1 --ntasks=4 --mem=16G"  # Slurm
  system: "pbs"  # or "slurm"
```

### 3. Submit Array Job
```bash
hsm sweep run --mode array --walltime "02:00:00"
```

### 4. Monitor Progress
```bash
hsm monitor --watch
```

## üìä Monitoring & Results

### Real-time Monitoring
```bash
# Monitor specific sweep
hsm monitor sweep_20241215_143022 --watch

# Monitor recent sweeps  
hsm recent --watch

# Monitor HPC queue
hsm queue --watch
```

### Collect Results
```bash
# Collect remote results
hsm collect-results sweep_20241215_143022

# Export results to CSV
hsm results sweep_20241215_143022 --format csv
```

## üîß Common Configuration

### HSM Configuration (`sweeps/hsm_config.yaml`)
```yaml
# Project paths
paths:
  python_interpreter: /opt/conda/bin/python
  train_script: train.py
  project_root: /path/to/project

# HPC settings
hpc:
  default_walltime: "04:00:00"  
  default_resources: "select=1:ncpus=4:mem=16gb"
  system: "pbs"

# W&B integration
wandb:
  project: my-experiment
  entity: my-team

# Distributed computing
distributed:
  enabled: true
  strategy: "round_robin"
  remotes:
    gpu-server:
      host: gpu.university.edu
      max_parallel_jobs: 2
```

### Sweep Configuration (`sweeps/sweep.yaml`)
```yaml
sweep:
  grid:
    seed: [1, 2, 3, 4, 5]
    model.hidden_size: [128, 256, 512]
    optimizer.lr: [0.001, 0.01, 0.1]
  
  paired:
    - model_and_data:
        model.name: [cnn, transformer]
        data.augmentation: [basic, advanced]

metadata:
  description: "Hyperparameter sweep for model comparison"
  tags: ["baseline", "comparison"]
```